{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve the OpenAI API key from the environment variables\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "\n",
    "# Import necessary libraries and modules for text processing and embeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_text_splitters import Language\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import chromadb\n",
    "import gradio as gr\n",
    "import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# METADATA FORMATTING\n",
    "\n",
    "# Function to read the content of a file\n",
    "\n",
    "def read_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return file.read()\n",
    "\n",
    "\n",
    "# Function to infer the module name based on the file path\n",
    "def infer_module_name(file_path):\n",
    "    path_parts = file_path.split(os.sep)\n",
    "    if \"src\" in path_parts:\n",
    "        src_index = path_parts.index(\"src\")\n",
    "        return \"/\".join(path_parts[src_index+1:-1])\n",
    "    return \"root\"\n",
    "\n",
    "\n",
    "# MAKING THE CONTENT FOR VECTOR EMBEDDINGS\n",
    "\n",
    "# Function to process files and create chunks of text for embeddings\n",
    "def process_files(root_dir, file_extension, language=None):\n",
    "    if language:\n",
    "        splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "            language=language, chunk_size=3000, chunk_overlap=100\n",
    "        )\n",
    "    else:\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=3000, chunk_overlap=100\n",
    "        )\n",
    "    \n",
    "    all_docs = []\n",
    "    # Walk through the directory to find and process files\n",
    "    for root, _, files in os.walk(root_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(file_extension):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_name = os.path.basename(file_path)\n",
    "                folder_path = root\n",
    "                module_name = infer_module_name(file_path)\n",
    "                content = read_file(file_path)\n",
    "                content = f\"file name: {file_name}\\n path: {folder_path}\\n {content}\"\n",
    "\n",
    "                docs = splitter.create_documents(\n",
    "                    [content],\n",
    "                    metadatas=[{\n",
    "                        'source': file_name, \n",
    "                        'type': file_extension[1:],\n",
    "                        'module': module_name,  \n",
    "                        'folder_path': folder_path  \n",
    "                    }]\n",
    "                )\n",
    "                all_docs.extend(docs)\n",
    "\n",
    "    return all_docs\n",
    "\n",
    "# To process files in the root directory for different file types and languages\n",
    "\n",
    "def process_all_files(root_directory):\n",
    "    ts_docs = process_files(root_directory, '.ts', Language.TS)\n",
    "    html_docs = process_files(root_directory, '.html', Language.HTML)\n",
    "    txt_docs = process_files(root_directory, '.txt')\n",
    "    md_docs = process_files(root_directory, '.md')\n",
    "    js_docs = process_files(root_directory, '.js', Language.JS)\n",
    "    kt_docs = process_files(root_directory, '.kt', Language.KOTLIN)\n",
    "\n",
    "    all_docs = ts_docs + html_docs + txt_docs + md_docs + js_docs + kt_docs\n",
    "    return all_docs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MAKE VECTOR EMBEDDINGS USING OPENAI EMBEDDING MODEL AND CHROMADB\n",
    "\n",
    "# Function to initialize or load an existing vector database\n",
    "\n",
    "def initialize_or_load_database():\n",
    "    model_name = 'text-embedding-3-large'   # Specify the OpenAI embedding model to use\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        model=model_name,\n",
    "        openai_api_key=os.environ.get('OPENAI_API_KEY')\n",
    "    )\n",
    "    \n",
    "    # Initializes a persistent ChromaDB client for storing vector embeddings\n",
    "    chroma_client = chromadb.PersistentClient(path=\"./mobile-dev_vector_storage\")\n",
    "    collection_name = \"all_files\"\n",
    "\n",
    "    # Check if a pre-existing collection is available (so that the vector storage need not be recreated every time the app is run)\n",
    "\n",
    "    if os.path.exists(\"collection_storage.txt\"):\n",
    "        with open(\"collection_storage.txt\", \"r\") as f:\n",
    "            collection_storage_name, collection_storage_id = f.read().splitlines()\n",
    "        print(\"Loading existing vector database...\")\n",
    "        docsearch = Chroma(\n",
    "            client=chroma_client,\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "    else:\n",
    "        print(\"Creating new vector database...\")\n",
    "        root_directory = \"mifos-mobile-development\"    # Directory containing the files to process  \n",
    "        all_documents = process_all_files(root_directory)\n",
    "        print(f\"Total number of chunks across all files: {len(all_documents)}\")\n",
    "        print(\"Total number of files: \", len(set([doc.metadata['source'] for doc in all_documents])))\n",
    "\n",
    "        docsearch = Chroma.from_documents(\n",
    "            documents=all_documents,\n",
    "            embedding=embeddings,\n",
    "            collection_name=collection_name,\n",
    "            client=chroma_client\n",
    "        )\n",
    "\n",
    "        collection_storage_name = chroma_client.list_collections()[0].name\n",
    "        collection_storage_id = chroma_client.list_collections()[0].id\n",
    "        print(\"name: \", collection_storage_name)\n",
    "        print(\"id: \", collection_storage_id)\n",
    "        \n",
    "        with open(\"collection_storage.txt\", \"w\") as f:\n",
    "            f.write(f\"{collection_storage_name}\\n{collection_storage_id}\")\n",
    "\n",
    "    return docsearch\n",
    "\n",
    "docsearch = initialize_or_load_database()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initializes the language model(GPT-4o-mini in this case) with specific parameters\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=os.environ.get('OPENAI_API_KEY'),\n",
    "    model_name='gpt-4o-mini',\n",
    "    temperature=0.3\n",
    ")\n",
    "\n",
    "# RetrievalQA chain with the language model and document retriever\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",  \n",
    "    retriever=docsearch.as_retriever(),\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# Function to get the top 20 document embeddings for a given query\n",
    "def get_top_20_embeddings(query):\n",
    "    docs_and_scores = docsearch.similarity_search_with_score(query, k=20) \n",
    "    return docs_and_scores\n",
    "\n",
    "# Retrieve document embeddings, ensuring that only unique parent documents are returned\n",
    "def get_parent_document_embeddings(query, num_docs=5):\n",
    "    docs_and_scores = docsearch.similarity_search_with_score(query, k=num_docs)\n",
    "    parent_docs = {}\n",
    "    \n",
    "    for doc, score in docs_and_scores:\n",
    "        parent_doc_key = doc.metadata['source'] \n",
    "        if parent_doc_key not in parent_docs:\n",
    "            parent_docs[parent_doc_key] = (doc, score)\n",
    "    \n",
    "    return list(parent_docs.values())\n",
    "\n",
    "def get_top_5_parent_documents(query):\n",
    "    return get_parent_document_embeddings(query, num_docs=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# PROMPT FOR THE CHATBOT TO UNDERSTAND THE CONTEXT AND THE QUESTION\n",
    "\n",
    "def answer_question_with_parent_docs(question):\n",
    "    top_5_results = get_top_5_parent_documents(question)\n",
    "    \n",
    "    context = \"\\n\".join([doc.page_content for doc, _ in top_5_results])\n",
    "    # print(\"Context: \", context)\n",
    "    \n",
    "    query_data = (\n",
    "        \"You are an expert in project structure and various file types including TypeScript, HTML, Markdown, JS and Kotlin.\"\n",
    "        \"When answering questions, focus on the file organization, key components of the codebase, and the structure of the project.\"\n",
    "        \"For general queries, provide a brief answer, but for questions about project structure, include module names, file paths, and folder organization.\"\n",
    "        \"If you're unsure of the answer, suggest referring to the Mifos Slack Channel.\"\n",
    "        \"\\nContext:\\n\" + context + \"\\n\" + question\n",
    "    )\n",
    "\n",
    "    response = qa.invoke(query_data)\n",
    "    # FOR REFERENCE AND DEBUGGING\n",
    "    # top_20_results = get_top_5_parent_documents(question)\n",
    "    # print(\"Top 5 matching parent documents:\")\n",
    "    # for i, (doc, score) in enumerate(top_20_results, 1):\n",
    "    #     print(f\"{i}. Document: {doc.page_content[:1000]}...\")\n",
    "    #     print(f\"   Metadata: {doc.metadata}\")\n",
    "    #     print(f\"   Similarity Score: {score}\")\n",
    "    #     print()\n",
    "    \n",
    "    return response['result']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# GRADIO INTERFACE FOR THE CHATBOT\n",
    "\n",
    "interface = gr.Interface(\n",
    "    fn=answer_question_with_parent_docs, \n",
    "    inputs=gr.Textbox(label=\"Ask a question about the files\"),\n",
    "    outputs=gr.Textbox(label=\"Answer\"),\n",
    "    title=\"Mifos-Mobile Chatbot\",\n",
    "    description=\"Ask questions about Kotlin in Mifos-Mobile\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    interface.launch(share=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
